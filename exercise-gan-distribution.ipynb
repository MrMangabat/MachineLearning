{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - GANs for learning distributions\n",
    "\n",
    "1. You are curious if GANs are able to model just *any* multivariate normal distribution, so you decide to try a new dimensionality, mean, and standard deviation, and otherwise replicate the study from the slides.\n",
    "1. As you mutter \"very well, it might work, but we also start with a standard normal - in reality we are newer so close to the true distribution to start off!\", you decide to model the noise from another distribution (such as the uniform distribution). Are you still able to model the true distribution? \n",
    "\n",
    "**Notes**: In this notebook, I have made a few choices (listed below). You **do not** have to follow those - you can use other distributions, dimensionalities, means, and standard deviations, if you like.\n",
    "1. The distribution to model is now the 20D normal distribution with mean $-5$ and standard deviation $-2$.\n",
    "1. The noise distribution for **2** is now the 20D uniform distribution (in the range $[0,1)$).\n",
    "\n",
    "**Hint**: Consider looking at https://www.tensorflow.org/tutorials/generative/dcgan, as they go through some of the same steps.\n",
    "\n",
    "**See slides for more details!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "You do not (but are of course welcome to) have to change any of the setup code.\n",
    "\n",
    "Note that we use 1 to indicate \"real\" data and zero to indicate \"fake\" data for the discriminator.\n",
    "\n",
    "In the loss of the generator, this is \"reversed\", i.e. fake data is 1. This is since it needs to learn to create fake data that the generator believes is real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def get_stats():\n",
    "    noise = tf.random.normal([1000, 20]) # standard normal\n",
    "    real = tf.random.normal([1000, 20], mean=-5, stddev=2) # to non-standard normal\n",
    "    \n",
    "    fake = generator.predict(noise)\n",
    "    \n",
    "    discr_real_pred = tf.nn.sigmoid(discriminator.predict(real)).numpy()\n",
    "    discr_fake_pred = tf.nn.sigmoid(discriminator.predict(fake)).numpy()\n",
    "    \n",
    "    acc_real = np.mean(discr_real_pred >= 0.5)\n",
    "    acc_fake = np.mean(discr_fake_pred < 0.5)\n",
    "                                    \n",
    "    return np.mean(fake), np.sqrt(np.var(fake)), acc_real, acc_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "You are curious if GANs are able to model just *any* multivariate normal distribution, so you decide to try a new dimensionality, mean, and standard deviation, and otherwise replicate the study from the slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by defining the generator and discriminator, as well as their optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us proceed by defining the training-step function we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step():\n",
    "    noise = tf.random.normal([32, 20]) # standard normal\n",
    "    real = tf.random.normal([32, 20], mean=-5, stddev=2) # REMEMBER THIS MUST MATCH!\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(real, training=True)\n",
    "        fake_output = discriminator(generated, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [('Before training', *get_stats())]\n",
    "\n",
    "for epoch in range(1, 20 + 1):\n",
    "    print(f'Starting epoch: {epoch}.')\n",
    "    \n",
    "    for _ in range(5000): # steps pr epoch\n",
    "        train_step()\n",
    "\n",
    "    results.append((epoch, *get_stats()))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Epoch', 'Mean', 'Std. Dev.', 'Acc., real', 'Acc., fake'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us create plots like in the slides to check that everything worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results_df['Mean'], label='Generator mean')\n",
    "plt.plot(results_df['Std. Dev.'], label='Generator std. dev.')\n",
    "plt.axhline(-5, color='r', label='True mean')\n",
    "plt.axhline(2, color='g', label='True std. dev.')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'),\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results_df['Acc., real'], label='Accuracy (real data)')\n",
    "plt.plot(results_df['Acc., fake'], label='Accuracy (fake data)')\n",
    "plt.plot(results_df[['Acc., real', 'Acc., fake']].mean(axis=1), label='Accuracy (overall)')\n",
    "plt.axhline(0.5, color='r', label='Nash equilibrium')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'),\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "As you mutter \"very well, it might work, but we also start with a standard normal - in reality we are newer so close to the true distribution to start off!\", you decide to model the noise from another distribution (such as the uniform distribution). Are you still able to model the true distribution? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this exercise, we must:\n",
    "1. Create a new generator and discriminator (including optimizers).\n",
    "1. Create a new get_stats function to use another noise distribution.\n",
    "1. Create a new train_step function to use the new noise distribution.\n",
    "1. And finally train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: New generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: New get_stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: New train_step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step():\n",
    "    noise = ??\n",
    "    real = ??\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(real, training=True)\n",
    "        fake_output = discriminator(generated, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Let us train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [('Before training', *get_stats())]\n",
    "\n",
    "for epoch in range(1, 20 + 1):\n",
    "    print(f'Starting epoch: {epoch}.')\n",
    "    \n",
    "    for _ in range(5000): # steps pr epoch\n",
    "        train_step()\n",
    "\n",
    "    results.append((epoch, *get_stats()))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Epoch', 'Mean', 'Std. Dev.', 'Acc., real', 'Acc., fake'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let us do the plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results_df['Mean'], label='Generator mean')\n",
    "plt.plot(results_df['Std. Dev.'], label='Generator std. dev.')\n",
    "plt.axhline(-5, color='r', label='True mean')\n",
    "plt.axhline(2, color='g', label='True std. dev.')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'),\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(results_df['Acc., real'], label='Accuracy (real data)')\n",
    "plt.plot(results_df['Acc., fake'], label='Accuracy (fake data)')\n",
    "plt.plot(results_df[['Acc., real', 'Acc., fake']].mean(axis=1), label='Accuracy (overall)')\n",
    "plt.axhline(0.5, color='r', label='Nash equilibrium')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'),\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
