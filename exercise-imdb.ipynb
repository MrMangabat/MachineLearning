{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - IMDB\n",
    "\n",
    "1. Use the IMDB movie review data (positive/negative movie reviews) to build a model that is able to predict the sentiment (positive/negative) from movie reviews. Your initial model should use one embedding layer, one recurrent layer (up to you which type), and a final fully connected layer to perform the classification.\n",
    "1. Try to improve your model by doing (at least) the following: add an additional recurrent layer and/or use a bidirectional recurrent layer (**note**: If you have a good 1-layer model this may be difficult - just try your best).\n",
    "1. In the preprocessing of the data made by me, I kepts the top 1000 words and let all reviews be 100 words long. Consider changing one/both of these to try to improve your best model (**hint**: the limit of only 100 words is very severe - try doubling it to 200, this may likely improve your performance).\n",
    "\n",
    "**Note**: You may want to use:\n",
    "1. https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN\n",
    "1. https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
    "1. https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU\n",
    "1. https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional\n",
    "1. https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data\n",
    "\n",
    "**See slides for more details!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_WORDS_KEEP = 1000 # keep top 1000 words (most occuring) -> all else cast to \"unknown\"\n",
    "SENTENCE_LEN = 100 # ensure sentences are exactly 100 words long. If longer, truncate. If shorter, pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'First sentence has length = {len(x_train[0])}')\n",
    "print(f'Second sentence has length = {len(x_train[1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above data is difficult to work with. Too many unique words and not uniform review length. Let's change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(\n",
    "    num_words=NB_WORDS_KEEP, # only keep 1000 most used words\n",
    "    start_char=1, # use 1 to indicate start of sentence\n",
    "    oov_char=2, # use 2 to indicate any word not in the top 1000\n",
    "    index_from=3 # start indexing words from 3\n",
    ")\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sample(sample, target_len):\n",
    "    if len(sample) > target_len:\n",
    "        return sample[:target_len] # if too long, shorten\n",
    "    if len(sample) < target_len: # if too short, pad\n",
    "        return sample + [0] * (target_len - len(sample)) # zero for these cases, i.e. padding\n",
    "    return sample\n",
    "\n",
    "def preprocess_imdb(x, target_len):\n",
    "    mod_x = []\n",
    "    \n",
    "    for sample in x:\n",
    "        mod_x.append(preprocess_sample(sample, target_len))\n",
    "        \n",
    "    return np.array(mod_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Use these modified sentences as the data for training and testing!\n",
    "z_train = preprocess_imdb(x_train, SENTENCE_LEN)\n",
    "z_test = preprocess_imdb(x_test, SENTENCE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'First sentence has length = {len(x_train[0])}')\n",
    "print(f'Second sentence has length = {len(x_train[1])}')\n",
    "\n",
    "print(f'First modified sentence has length = {len(z_train[0])}')\n",
    "print(f'Second modified sentence has length = {len(z_train[1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Use the IMDB movie review data (positive/negative movie reviews) to build a model that is able to predict the sentiment (positive/negative) from movie reviews. Your initial model should use one embedding layer, one recurrent layer (up to you which type), and a final fully connected layer to perform the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=??, output_dim=??),\n",
    "    tf.keras.layers.??(??),\n",
    "    tf.keras.layers.Dense(??, activation=??),\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss=??,\n",
    "    optimizer=??,\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Try to improve your model by doing (at least) the following: add an additional recurrent layer and/or use a bidirectional recurrent layer (**note**: If you have a good 1-layer model this may be difficult - just try your best)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deep = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=??, output_dim=??),\n",
    "    tf.keras.layers.LSTM(??, return_sequences=Â´??),\n",
    "    tf.keras.layers.LSTM(??),\n",
    "    tf.keras.layers.Dense(??, activation=??),\n",
    "])\n",
    "\n",
    "model_deep.summary()\n",
    "\n",
    "model_deep.compile(\n",
    "    ??\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_deep.fit(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bidirectional = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=??, output_dim=??),\n",
    "    tf.keras.layers.Bidirectional(??),\n",
    "    tf.keras.layers.Dense(??),\n",
    "])\n",
    "\n",
    "model_bidirectional.summary()\n",
    "\n",
    "model_bidirectional.compile(\n",
    "    ??\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bidirectional.fit(??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "In the preprocessing of the data made by me, I kepts the top 1000 words and let all reviews be 100 words long. Consider changing one/both of these to try to improve your best model (**hint**: the limit of only 100 words is very severe - try doubling it to 200, this may likely improve your performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On your own here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
