{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - AEs\n",
    "\n",
    "Use the $\\texttt{ECG5000}$ dataset$^1$ to perform autoencoding and anomaly detection of ECGs. Specifically, split the data into normal and anomalous data as well as a train and test set. \n",
    "\n",
    "1. Use a linear and a nonlinear autoencoder to perform reconstructions of the **normal** ECGs. Measure performance both by MSE and MAE. Which model is best on the respective measures (measured on the test data, having used the train data to train the models)? The bottleneck layer should contain 8 neurons.\n",
    "1. Use one or both of the models from above to perform anomaly detection (you decide which metric, i.e. MSE or MAE, to use for this purpose). That is, find the losses on the training data, and then decide on a threshold above which you classify data as anomolous.\n",
    "1. Use a supervised model to perform anomaly detection (i.e. use both the normal and anomolous data for training). Is this model better than the approach above? Is this still the case if you restrict the number of anomalies in the training data to a small number (such as 10)?\n",
    "\n",
    "$^1$http://www.timeseriesclassification.com/description.php?Dataset=ECG5000.\n",
    "\n",
    "**Hint**: Consider looking at https://www.tensorflow.org/tutorials/generative/autoencoder, as they go through some of the same steps.\n",
    "\n",
    "**See slides for more details!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "This is simply some code to prepare the data. Mostly similar to https://www.tensorflow.org/tutorials/generative/autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "dataframe = pd.read_csv('http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv', header=None)\n",
    "raw_data = dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last element contains the labels\n",
    "labels = raw_data[:, -1]\n",
    "\n",
    "# The other data points are the electrocadriogram data\n",
    "data = raw_data[:, 0:-1]\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    data, labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "\n",
    "x_train = scaler.transform(train_data)\n",
    "x_test = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normal = x_train[train_labels == 1]\n",
    "x_train_anomalous = x_train[train_labels == 0]\n",
    "\n",
    "x_test_normal = x_test[test_labels == 1]\n",
    "x_test_anomalous = x_test[test_labels == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_normal.shape, x_train_anomalous.shape, x_test_normal.shape, x_test_anomalous.shape, train_labels.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "plt.plot(train_data[train_labels == 1][0])\n",
    "plt.title(\"A Normal ECG\")\n",
    "plt.show()\n",
    "\n",
    "plt.grid()\n",
    "plt.plot(train_data[train_labels == 0][0])\n",
    "plt.title(\"An Anomolous ECG\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Use a linear and a nonlinear autoencoder to perform reconstructions of the **normal** ECGs. Measure performance both by MSE and MAE. Which model is best on the respective measures (measured on the test data, having used the train data to train the models)? The bottleneck layer should contain 8 neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the linear autoencoder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_linear = tf.keras.models.Sequential([\n",
    "    ??\n",
    "], name='encoder')\n",
    "\n",
    "decoder_linear = tf.keras.models.Sequential([\n",
    "    ??\n",
    "], name='decoder')\n",
    "\n",
    "autoencoder_linear = tf.keras.models.Sequential([\n",
    "    ??\n",
    "], name='autoencoder')\n",
    "\n",
    "autoencoder_linear.compile(loss=??, optimizer=??, metrics=[??])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_linear.summary()\n",
    "print('\\n')\n",
    "decoder_linear.summary()\n",
    "print('\\n')\n",
    "autoencoder_linear.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the nonlinear autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_nonlinear = tf.keras.models.Sequential([\n",
    "    ??\n",
    "], name='encoder')\n",
    "\n",
    "decoder_nonlinear = tf.keras.models.Sequential([\n",
    "    ??\n",
    "], name='decoder')\n",
    "\n",
    "autoencoder_nonlinear = tf.keras.models.Sequential([\n",
    "    ??\n",
    "], name='autoencoder')\n",
    "\n",
    "autoencoder_nonlinear.compile(loss=??, optimizer=??, metrics=[??])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_nonlinear.summary()\n",
    "print('\\n')\n",
    "decoder_nonlinear.summary()\n",
    "print('\\n')\n",
    "autoencoder_nonlinear.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hist_linear = autoencoder_linear.fit(\n",
    "    ??\n",
    ")\n",
    "\n",
    "hist_nonlinear = autoencoder_nonlinear.fit(\n",
    "    ??\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_linear.history[\"loss\"], label=\"Training MSE (linear)\")\n",
    "plt.plot(hist_linear.history[\"val_loss\"], label=\"Test MSE (linear)\")\n",
    "plt.plot(hist_nonlinear.history[\"loss\"], label=\"Training MSE (nonlinear)\")\n",
    "plt.plot(hist_nonlinear.history[\"val_loss\"], label=\"Test MSE (nonlinear)\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist_linear.history[\"mae\"], label=\"Training MAE (linear)\")\n",
    "plt.plot(hist_linear.history[\"val_mae\"], label=\"Test MAE (linear)\")\n",
    "plt.plot(hist_nonlinear.history[\"mae\"], label=\"Training MAE (nonlinear)\")\n",
    "plt.plot(hist_nonlinear.history[\"val_mae\"], label=\"Test MAE (nonlinear)\")\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Use one or both of the models from above to perform anomaly detection (you decide which metric, i.e. MSE or MAE, to use for this purpose). That is, find the losses on the training data, and then decide on a threshold above which you classify data as anomolous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find the non-anomaly training and test reconstructions losses for each network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normal_reconstructed_linear = autoencoder_linear.predict(x_train_normal)\n",
    "x_train_normal_reconstructed_nonlinear = autoencoder_nonlinear.predict(x_train_normal)\n",
    "\n",
    "mse_x_train_normal_linear = tf.keras.losses.mse(x_train_normal_reconstructed_linear, x_train_normal)\n",
    "mae_x_train_normal_linear = tf.keras.losses.mae(x_train_normal_reconstructed_linear, x_train_normal)\n",
    "mse_x_train_normal_nonlinear = tf.keras.losses.mse(x_train_normal_reconstructed_nonlinear, x_train_normal)\n",
    "mae_x_train_normal_nonlinear = tf.keras.losses.mae(x_train_normal_reconstructed_nonlinear, x_train_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_normal_reconstructed_linear = autoencoder_linear.predict(x_test_normal)\n",
    "x_test_normal_reconstructed_nonlinear = autoencoder_nonlinear.predict(x_test_normal)\n",
    "\n",
    "mse_x_test_normal_linear = tf.keras.losses.mse(x_test_normal_reconstructed_linear, x_test_normal)\n",
    "mae_x_test_normal_linear = tf.keras.losses.mae(x_test_normal_reconstructed_linear, x_test_normal)\n",
    "mse_x_test_normal_nonlinear = tf.keras.losses.mse(x_test_normal_reconstructed_nonlinear, x_test_normal)\n",
    "mae_x_test_normal_nonlinear = tf.keras.losses.mae(x_test_normal_reconstructed_nonlinear, x_test_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us find all the non-anomaly (**note**: here we can use train *and* test as the test set here, as we never actually used the training data for anything)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_anomalous = np.concatenate([x_train_anomalous, x_test_anomalous])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_anomalous_reconstructed_linear = autoencoder_linear.predict(x_anomalous)\n",
    "x_anomalous_reconstructed_nonlinear = autoencoder_nonlinear.predict(x_anomalous)\n",
    "\n",
    "mse_x_anomalous_linear = tf.keras.losses.mse(x_anomalous_reconstructed_linear, x_anomalous)\n",
    "mae_x_anomalous_linear = tf.keras.losses.mae(x_anomalous_reconstructed_linear, x_anomalous)\n",
    "mse_x_anomalous_nonlinear = tf.keras.losses.mse(x_anomalous_reconstructed_nonlinear, x_anomalous)\n",
    "mae_x_anomalous_nonlinear = tf.keras.losses.mae(x_anomalous_reconstructed_nonlinear, x_anomalous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to determine a \"cutoff\" or \"threshold\" value, above which we classify an observation as an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Use a supervised model to perform anomaly detection (i.e. use both the normal and anomolous data for training). Is this model better than the approach above? Is this still the case if you restrict the number of anomalies in the training data to a small number (such as 10)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_anomaly_detecter = tf.keras.models.Sequential([\n",
    "    ??\n",
    "])\n",
    "\n",
    "supervised_anomaly_detecter.compile(??)\n",
    "\n",
    "supervised_anomaly_detecter.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_supervised = supervised_anomaly_detecter.fit(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_supervised.history[\"loss\"], label=\"Training loss\")\n",
    "plt.plot(hist_supervised.history[\"val_loss\"], label=\"Test loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist_supervised.history[\"accuracy\"], label=\"Training accuracy\")\n",
    "plt.plot(hist_supervised.history[\"val_accuracy\"], label=\"Test accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same model, but lower number of anomalies in the training data.\n",
    "\n",
    "We will use the same test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_anomaly_detecter_2 = tf.keras.models.Sequential([\n",
    "    ??\n",
    "])\n",
    "\n",
    "supervised_anomaly_detecter_2.compile(??)\n",
    "\n",
    "supervised_anomaly_detecter_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hist_supervised_2 = supervised_anomaly_detecter_2.fit(\n",
    "    x=np.concatenate([x_train_normal, x_train_anomalous[:10]]), # 10 examples\n",
    "    y=np.concatenate([train_labels[train_labels == 1], train_labels[train_labels == 0][:10]]), \n",
    "    validation_data=(x_test, test_labels), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_supervised_2.history[\"loss\"], label=\"Training loss\")\n",
    "plt.plot(hist_supervised_2.history[\"val_loss\"], label=\"Test loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist_supervised_2.history[\"accuracy\"], label=\"Training accuracy\")\n",
    "plt.plot(hist_supervised_2.history[\"val_accuracy\"], label=\"Test accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
